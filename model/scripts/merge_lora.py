import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel, PeftConfig

# ==== é…ç½®è·¯å¾„ï¼ˆè¯·ç¡®ä¿è·¯å¾„æ­£ç¡®ï¼‰ ====
base_model_path = "../FlagAlphaLlama3-Chinese-8B-Instruct"  # åŸºç¡€æ¨¡å‹è·¯å¾„
adapter_path = "../models/lora_finetuned"                  # LoRAé€‚é…å™¨è·¯å¾„
output_path = "../models/merged_model"                     # åˆå¹¶åè¾“å‡ºè·¯å¾„

try:
    # ==== åŠ è½½åŸºç¡€æ¨¡å‹ ====
    print(f"ğŸ” æ­£åœ¨åŠ è½½åŸºç¡€æ¨¡å‹: {base_model_path}")
    model = AutoModelForCausalLM.from_pretrained(
        base_model_path,
        torch_dtype=torch.float16,        # æ˜ç¡®æŒ‡å®šç²¾åº¦
        device_map="auto",
        trust_remote_code=True            # å…³é”®å‚æ•°ï¼šå…è®¸è‡ªå®šä¹‰æ¨¡å‹ä»£ç 
    )
    
    # ==== åŠ è½½tokenizer ====
    tokenizer = AutoTokenizer.from_pretrained(
        base_model_path,
        trust_remote_code=True
    )

    # ==== åŠ è½½LoRAé€‚é…å™¨ ====
    print(f"ğŸ”„ æ­£åœ¨åŠ è½½LoRAé€‚é…å™¨: {adapter_path}")
    
    # å…ˆæ£€æŸ¥é€‚é…å™¨é…ç½®
    adapter_config = PeftConfig.from_pretrained(adapter_path)
    print(f"é€‚é…å™¨é…ç½®: {adapter_config}")
    
    # åŠ è½½é€‚é…å™¨ï¼ˆæ·»åŠ is_trainable=Falseé¿å…æ„å¤–è®­ç»ƒï¼‰
    model = PeftModel.from_pretrained(
        model, 
        adapter_path,
        is_trainable=False
    )

    # ==== åˆå¹¶æ¨¡å‹ ====
    print("âš¡ æ­£åœ¨åˆå¹¶LoRAé€‚é…å™¨...")
    merged_model = model.merge_and_unload()  # æ³¨æ„ä½¿ç”¨è¿”å›å€¼
    
    # ==== ä¿å­˜åˆå¹¶åçš„æ¨¡å‹ ====
    print(f"ğŸ’¾ æ­£åœ¨ä¿å­˜æ¨¡å‹åˆ°: {output_path}")
    merged_model.save_pretrained(
        output_path,
        safe_serialization=True  # å®‰å…¨åºåˆ—åŒ–
    )
    tokenizer.save_pretrained(output_path)
    
    print("âœ… åˆå¹¶å®Œæˆï¼åˆå¹¶åçš„æ¨¡å‹å·²ä¿å­˜è‡³:", output_path)

except Exception as e:
    print(f"âŒ å‘ç”Ÿé”™è¯¯: {str(e)}")
    # æ‰“å°å®Œæ•´é”™è¯¯ä¿¡æ¯ï¼ˆè°ƒè¯•ç”¨ï¼‰
    import traceback
    traceback.print_exc()