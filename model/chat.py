import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

class EnhancedLlamaChat:
    def __init__(self):
        self.MODEL_PATH = "models/merged_model"
        self._init_device()
        self._load_model()
        self._warmup()

    def _init_device(self):
        """åˆå§‹åŒ–è®¾å¤‡é…ç½®"""
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"ğŸ› ï¸ è¿è¡Œè®¾å¤‡: {self.device.upper()}")
        if self.device == "cuda":
            print(f"  æ˜¾å¡å‹å·: {torch.cuda.get_device_name(0)}")
            print(f"  CUDAå†…å­˜: {torch.cuda.get_device_properties(0).total_memory/1e9:.1f}GB")

    def _load_model(self):
        """åŠ è½½æ¨¡å‹å’Œtokenizer"""
        print("ğŸ”Œ æ­£åœ¨åŠ è½½æ¨¡å‹...")
        try:
            self.model = AutoModelForCausalLM.from_pretrained(
                self.MODEL_PATH,
                device_map="auto",  # è‡ªåŠ¨åˆ†é…GPU/CPU
                torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
                trust_remote_code=True
            )
            
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.MODEL_PATH,
                trust_remote_code=True
            )
            
            # è®¾ç½®pad_token
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
                print(f"â„¹ï¸ è®¾ç½®pad_token: {self.tokenizer.pad_token}")
            
            total_params = sum(p.numel() for p in self.model.parameters())
            print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ | å‚æ•°é‡: {total_params/1e9:.2f}B")

        except Exception as e:
            print(f"âŒ åŠ è½½å¤±è´¥: {str(e)}")
            raise

    def _warmup(self):
        """é¢„çƒ­æ¨¡å‹"""
        print("ğŸ”¥ æ¨¡å‹é¢„çƒ­ä¸­...")
        test_prompt = "æ¨¡å‹é¢„çƒ­"
        inputs = self.tokenizer(test_prompt, return_tensors="pt").to(self.device)
        with torch.inference_mode():
            _ = self.model.generate(**inputs, max_new_tokens=10)
        if self.device == "cuda":
            torch.cuda.empty_cache()

    def generate(self, prompt, **kwargs):
        """ç”Ÿæˆæ–‡æœ¬"""
        inputs = self.tokenizer(
        prompt,
        return_tensors="pt",
        padding=True,
        truncation=True,
        max_length=512
        ).to(self.device)

        default_generate_args = {
        "max_new_tokens": 256,
        "do_sample": True,              # é»˜è®¤å¯ç”¨é‡‡æ ·æ¨¡å¼
        "top_k": 50,
        "top_p": 0.9,
        "temperature": 0.7,
        "repetition_penalty": 1.1,
        }

        # åˆå¹¶é»˜è®¤å‚æ•°ä¸å¤–éƒ¨ä¼ å…¥çš„ kwargsï¼ˆå¤–éƒ¨ä¼˜å…ˆï¼‰
        generate_args = {**default_generate_args, **kwargs}

        with torch.inference_mode():
            output = self.model.generate(
                **inputs,
                **generate_args
            )

            return self.tokenizer.decode(output[0], skip_special_tokens=True)   


    def interactive_chat(self):
        """äº¤äº’å¼å¯¹è¯"""
        print("\nğŸ’¬ è¿›å…¥å¯¹è¯æ¨¡å¼ (è¾“å…¥ 'quit' é€€å‡º)")
        while True:
            try:
                user_input = input("\nğŸ‘¤ æ‚¨: ")
                if user_input.lower() in ['quit', 'exit']:
                    break
                
                response = self.generate(
                    f"<|User|>{user_input}<|Bot|>",
                    temperature=0.8
                ).split("<|Bot|>")[-1].strip()
                
                print(f"\nğŸ¤– AI: {response}")
                
            except KeyboardInterrupt:
                print("\nğŸ›‘ å¯¹è¯ç»ˆæ­¢")
                break

if __name__ == "__main__":
    bot = EnhancedLlamaChat()
    
    # å¿«é€Ÿæµ‹è¯•
    print("\nğŸš€ æµ‹è¯•ç”Ÿæˆ:")
    test_q = "Where is the captical of Americanï¼Ÿ"
    print(f"Q: {test_q}")
    print(f"A: {bot.generate(test_q)}")
    
    # å¼€å¯å¯¹è¯
    bot.interactive_chat()
